"""
Zabbix AI Assistant - Streamlitç‰ˆ
share.streamlit.io å¯¾å¿œ
"""

import streamlit as st
import json
import os
import re
import hashlib
from datetime import datetime, timedelta
import random
import requests

# ==================== ãƒšãƒ¼ã‚¸è¨­å®š ====================
st.set_page_config(
    page_title="Zabbix AI Assistant",
    page_icon="ğŸ–¥ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== ã‚«ã‚¹ã‚¿ãƒ CSS ====================
st.markdown("""
<style>
    /* ãƒ€ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒé¢¨ */
    .stApp {
        background-color: #0a0e14;
        color: #f0f0f0;
    }
    
    /* å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆè‰²ã‚’æ˜ã‚‹ã */
    .stApp, .stApp p, .stApp span, .stApp div, .stApp li {
        color: #f0f0f0 !important;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ */
    [data-testid="stSidebar"] {
        background-color: #151b23;
    }
    [data-testid="stSidebar"] * {
        color: #f0f0f0 !important;
    }
    
    /* ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ */
    .stChatMessage {
        background-color: #1e2630;
        border-radius: 10px;
        padding: 10px;
    }
    .stChatMessage p, .stChatMessage span, .stChatMessage div {
        color: #f0f0f0 !important;
    }
    
    /* ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ */
    .stMarkdown, .stMarkdown p, .stMarkdown span {
        color: #f0f0f0 !important;
    }
    
    /* ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ */
    .stCaption, small {
        color: #b0b0b0 !important;
    }
    
    /* ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ãƒ¼ãƒ‰ */
    .metric-card {
        background: linear-gradient(135deg, #1e2630, #151b23);
        border-radius: 10px;
        padding: 15px;
        margin: 5px 0;
        border-left: 3px solid #00ff9d;
    }
    
    /* ã‚¢ãƒ©ãƒ¼ãƒˆã‚«ãƒ¼ãƒ‰ */
    .alert-high {
        border-left: 3px solid #ff4757;
    }
    .alert-warning {
        border-left: 3px solid #ffc107;
    }
    
    /* ãƒœã‚¿ãƒ³ */
    .stButton > button {
        background: linear-gradient(135deg, #00ff9d, #00b8ff);
        color: #0a0e14 !important;
        font-weight: bold;
        border: none;
    }
    
    /* ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ */
    .stCodeBlock {
        background-color: #0a0e14 !important;
    }
    
    /* ãƒ˜ãƒƒãƒ€ãƒ¼éè¡¨ç¤º */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ã‚¿ã‚¤ãƒ« */
    .main-title {
        background: linear-gradient(135deg, #00ff9d, #00b8ff);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 2em;
        font-weight: bold;
    }
    
    /* ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒƒã‚¸ */
    .cache-badge {
        background: #ffc107;
        color: #0a0e14;
        padding: 2px 8px;
        border-radius: 10px;
        font-size: 12px;
        margin-left: 10px;
    }
    
    /* å…¥åŠ›æ¬„ */
    .stTextInput input, .stChatInput input, .stChatInput textarea {
        color: #f0f0f0 !important;
        background-color: #1e2630 !important;
    }
    
    /* expander */
    .streamlit-expanderHeader {
        color: #f0f0f0 !important;
    }
</style>
""", unsafe_allow_html=True)

# ==================== ãƒ‡ãƒ¼ã‚¿ç®¡ç† ====================

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")

@st.cache_data
def load_topology():
    """ãƒˆãƒãƒ­ã‚¸ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    topology_path = os.path.join(DATA_DIR, "topology.json")
    if os.path.exists(topology_path):
        with open(topology_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

@st.cache_data
def load_mock_data():
    """ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    mock_path = os.path.join(DATA_DIR, "mock_data.json")
    if os.path.exists(mock_path):
        with open(mock_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"hosts": {}, "alerts": [], "metrics_history": {}, "maintenance": {}}

def get_hosts():
    """ãƒ›ã‚¹ãƒˆä¸€è¦§ã‚’å–å¾—"""
    data = load_mock_data()
    return data.get("hosts", {})

def get_alerts():
    """ã‚¢ãƒ©ãƒ¼ãƒˆä¸€è¦§ã‚’å–å¾—"""
    data = load_mock_data()
    return data.get("alerts", [])

def get_hosts_by_condition(metric: str, operator: str, value: float) -> list:
    """æ¡ä»¶ã«åˆã†ãƒ›ã‚¹ãƒˆã‚’å–å¾—"""
    hosts = get_hosts()
    results = []
    for host_id, host in hosts.items():
        if metric in host.get("metrics", {}):
            current_value = host["metrics"][metric]
            if operator == ">" and current_value > value:
                results.append({"host_id": host_id, **host, "current_value": current_value})
            elif operator == "<" and current_value < value:
                results.append({"host_id": host_id, **host, "current_value": current_value})
            elif operator == "=" and current_value == value:
                results.append({"host_id": host_id, **host, "current_value": current_value})
    return sorted(results, key=lambda x: x["current_value"], reverse=True)

def get_host_metrics(host_id: str) -> dict:
    """ãƒ›ã‚¹ãƒˆã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
    hosts = get_hosts()
    if host_id in hosts:
        return hosts[host_id].get("metrics", {})
    return {}

def generate_metrics_history(host_id: str, metric: str, hours: int = 24) -> list:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã‚’ç”Ÿæˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰"""
    hosts = get_hosts()
    base_value = hosts.get(host_id, {}).get("metrics", {}).get(metric, 50)
    history = []
    now = datetime.now()
    
    for i in range(hours * 6):  # 10åˆ†é–“éš”
        timestamp = now - timedelta(minutes=10 * (hours * 6 - i))
        hour = timestamp.hour
        # 14æ™‚é ƒã«æ€¥ä¸Šæ˜‡ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        if 14 <= hour <= 16:
            spike = random.uniform(20, 40)
        else:
            spike = 0
        value = base_value + random.uniform(-10, 10) + spike
        value = max(0, min(100, value))
        history.append({
            "timestamp": timestamp,
            "value": round(value, 1)
        })
    return history

def generate_zabbix_config(topology: dict) -> dict:
    """ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‹ã‚‰Zabbixè¨­å®šã‚’ç”Ÿæˆ"""
    config = {
        "host_groups": [],
        "hosts": [],
        "templates": [],
        "triggers": [],
        "dependencies": []
    }
    
    # ãƒ›ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ç”Ÿæˆ
    layers = set()
    vendors = set()
    locations = set()
    ha_groups = set()
    
    for host_id, host_data in topology.items():
        layers.add(f"Layer{host_data['layer']}")
        if host_data.get("metadata", {}).get("vendor"):
            vendors.add(host_data["metadata"]["vendor"])
        if host_data.get("metadata", {}).get("location"):
            locations.add(host_data["metadata"]["location"])
        if host_data.get("redundancy_group"):
            ha_groups.add(host_data["redundancy_group"])
            
    config["host_groups"] = [
        *[{"name": f"Network/{layer}", "type": "layer"} for layer in sorted(layers)],
        *[{"name": f"Vendor/{vendor}", "type": "vendor"} for vendor in vendors],
        *[{"name": f"Location/{loc}", "type": "location"} for loc in locations],
        *[{"name": f"HA_Groups/{group}", "type": "ha"} for group in ha_groups]
    ]
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒ”ãƒ³ã‚°
    template_map = {
        ("Cisco", "ROUTER"): ["Template Cisco IOS-XE SNMP", "Template ICMP Ping"],
        ("Cisco", "SWITCH"): ["Template Cisco Catalyst SNMP", "Template ICMP Ping"],
        ("Juniper", "FIREWALL"): ["Template Juniper SRX SNMP", "Template ICMP Ping"],
        ("default", "ACCESS_POINT"): ["Template Generic SNMP AP", "Template ICMP Ping"],
    }
    
    # ãƒ›ã‚¹ãƒˆè¨­å®šç”Ÿæˆ
    for host_id, host_data in topology.items():
        vendor = host_data.get("metadata", {}).get("vendor", "default")
        device_type = host_data.get("type", "unknown")
        
        templates = template_map.get((vendor, device_type), 
                    template_map.get(("default", device_type), ["Template ICMP Ping"]))
        
        groups = [f"Network/Layer{host_data['layer']}"]
        if vendor != "default":
            groups.append(f"Vendor/{vendor}")
        if host_data.get("metadata", {}).get("location"):
            groups.append(f"Location/{host_data['metadata']['location']}")
        if host_data.get("redundancy_group"):
            groups.append(f"HA_Groups/{host_data['redundancy_group']}")
        
        host_config = {
            "host_id": host_id,
            "name": host_id,
            "groups": groups,
            "templates": templates,
            "tags": [
                {"tag": "layer", "value": str(host_data["layer"])},
                {"tag": "type", "value": device_type},
            ],
            "macros": {}
        }
        
        if vendor != "default":
            host_config["tags"].append({"tag": "vendor", "value": vendor})
        if host_data.get("metadata", {}).get("model"):
            host_config["tags"].append({"tag": "model", "value": host_data["metadata"]["model"]})
        if host_data.get("metadata", {}).get("hw_inventory", {}).get("psu_count"):
            host_config["macros"]["{$PSU_COUNT}"] = host_data["metadata"]["hw_inventory"]["psu_count"]
            
        config["hosts"].append(host_config)
        
        # ä¾å­˜é–¢ä¿‚è¨­å®š
        if host_data.get("parent_id"):
            config["dependencies"].append({
                "host": host_id,
                "depends_on": host_data["parent_id"],
                "type": "parent"
            })
    
    # ãƒˆãƒªã‚¬ãƒ¼ç”Ÿæˆ
    for host_id, host_data in topology.items():
        device_type = host_data.get("type", "unknown")
        
        config["triggers"].append({
            "host": host_id,
            "name": f"{host_id} is unreachable",
            "expression": f"nodata(/{host_id}/icmp.ping,5m)=1",
            "severity": "high" if host_data["layer"] <= 2 else "average"
        })
        
        if device_type in ["ROUTER", "SWITCH", "FIREWALL"]:
            config["triggers"].append({
                "host": host_id,
                "name": f"{host_id} CPU usage is high",
                "expression": f"last(/{host_id}/system.cpu.util)>80",
                "severity": "warning"
            })
            
        if host_data.get("redundancy_group"):
            config["triggers"].append({
                "host": host_id,
                "name": f"HA Failover detected - {host_id}",
                "expression": f"change(/{host_id}/ha.role,1h)<>0",
                "severity": "warning"
            })
    
    return config

# ==================== ã‚³ãƒãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ====================

def get_cache_key(intent: str) -> str:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    return hashlib.md5(intent.lower().strip().encode()).hexdigest()

def get_command_cache():
    """ã‚³ãƒãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
    if "command_cache" not in st.session_state:
        st.session_state.command_cache = {}
    return st.session_state.command_cache

def set_command_cache(intent: str, command: dict):
    """ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache = get_command_cache()
    key = get_cache_key(intent)
    cache[key] = {
        "intent": intent,
        "command": command,
        "created_at": datetime.now().isoformat(),
        "use_count": 1
    }

def get_cached_command(intent: str):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚³ãƒãƒ³ãƒ‰ã‚’å–å¾—"""
    cache = get_command_cache()
    key = get_cache_key(intent)
    if key in cache:
        cache[key]["use_count"] += 1
        return cache[key]["command"]
    return None

# ==================== LLMé€£æº ====================

def call_gemini(user_message: str) -> dict:
    """Google AI Studio APIã‚’å‘¼ã³å‡ºã™"""
    api_key = st.secrets.get("GOOGLE_API_KEY", "") if hasattr(st, 'secrets') else os.getenv("GOOGLE_API_KEY", "")
    
    if not api_key:
        return generate_mock_response(user_message)
    
    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemma-3-12b-it:generateContent?key={api_key}"
        
        system_prompt = """ã‚ãªãŸã¯Zabbixç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’è§£æã—ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„:
{"intent": "æ„å›³", "action": "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å", "parameters": {ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿}}

ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: generate_config, set_maintenance, search_hosts, get_metrics, get_alerts, show_graph"""
        
        payload = {
            "contents": [{"parts": [{"text": f"{system_prompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_message}"}]}],
            "generationConfig": {"temperature": 0.7, "maxOutputTokens": 1024}
        }
        
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        
        text = result["candidates"][0]["content"]["parts"][0]["text"]
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            return json.loads(json_match.group())
    except Exception as e:
        st.warning(f"APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return generate_mock_response(user_message)

def generate_mock_response(user_message: str) -> dict:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ¢ãƒƒã‚¯å¿œç­”"""
    message_lower = user_message.lower()
    
    if "ãƒˆãƒãƒ­ã‚¸ãƒ¼" in message_lower and ("è¨­å®š" in message_lower or "ç›£è¦–" in message_lower):
        return {"intent": "ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‹ã‚‰Zabbixè¨­å®šã‚’ç”Ÿæˆ", "action": "generate_config", "parameters": {}}
    
    if "ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹" in message_lower:
        host_match = re.search(r'([A-Z][A-Z0-9_-]+)', user_message)
        host_id = host_match.group(1) if host_match else "WAN_ROUTER_01"
        time_match = re.search(r'(\d+)\s*(åˆ†|æ™‚é–“|hour|min)', message_lower)
        duration = 60
        if time_match:
            duration = int(time_match.group(1))
            if "æ™‚é–“" in time_match.group(2) or "hour" in time_match.group(2):
                duration *= 60
        return {"intent": f"{host_id}ã‚’ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š", "action": "set_maintenance", "parameters": {"host_id": host_id, "duration_minutes": duration}}
    
    if "cpu" in message_lower and ("é«˜ã„" in message_lower or "è¶…ãˆ" in message_lower or ">" in message_lower):
        threshold_match = re.search(r'(\d+)\s*%?', user_message)
        threshold = int(threshold_match.group(1)) if threshold_match else 80
        return {"intent": f"CPU{threshold}%è¶…ãˆã®ãƒ›ã‚¹ãƒˆã‚’æ¤œç´¢", "action": "search_hosts", "parameters": {"metric": "cpu", "operator": ">", "value": threshold}}
    
    if "ãƒ¡ãƒˆãƒªã‚¯ã‚¹" in message_lower or "çŠ¶æ…‹" in message_lower:
        host_match = re.search(r'([A-Z][A-Z0-9_-]+)', user_message)
        if host_match:
            return {"intent": f"{host_match.group(1)}ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—", "action": "get_metrics", "parameters": {"host_id": host_match.group(1)}}
    
    if "ã‚¢ãƒ©ãƒ¼ãƒˆ" in message_lower or "éšœå®³" in message_lower or "å•é¡Œ" in message_lower:
        return {"intent": "ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆä¸€è¦§ã‚’å–å¾—", "action": "get_alerts", "parameters": {}}
    
    if "ã‚°ãƒ©ãƒ•" in message_lower or "æ¨ç§»" in message_lower or "ãƒˆãƒ¬ãƒ³ãƒ‰" in message_lower:
        host_match = re.search(r'([A-Z][A-Z0-9_-]+)', user_message)
        host_id = host_match.group(1) if host_match else "WAN_ROUTER_01"
        metric = "cpu" if "cpu" in message_lower else "memory" if "ãƒ¡ãƒ¢ãƒª" in message_lower else "cpu"
        return {"intent": f"{host_id}ã®{metric}ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º", "action": "show_graph", "parameters": {"host_id": host_id, "metric": metric, "hours": 24}}
    
    return {"intent": "ä¸æ˜", "action": "unknown", "parameters": {"original_query": user_message}}

# ==================== ãƒ¡ã‚¤ãƒ³å‡¦ç† ====================

def process_message(user_message: str) -> tuple:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ"""
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cached = get_cached_command(user_message)
    if cached:
        response = cached
        is_cached = True
    else:
        response = call_gemini(user_message)
        is_cached = False
        if response.get("action") != "unknown":
            set_command_cache(user_message, response)
    
    action = response.get("action", "unknown")
    params = response.get("parameters", {})
    
    result = {"response": response, "cached": is_cached}
    
    if action == "generate_config":
        topology = load_topology()
        if not topology:
            result["message"] = "âŒ ãƒˆãƒãƒ­ã‚¸ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
        else:
            config = generate_zabbix_config(topology)
            result["config"] = config
            result["message"] = f"""âœ… Zabbixè¨­å®šã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼š
â€¢ ãƒ›ã‚¹ãƒˆ: {len(config['hosts'])}å°
â€¢ ãƒ›ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—: {len(config['host_groups'])}å€‹
â€¢ ãƒˆãƒªã‚¬ãƒ¼: {len(config['triggers'])}å€‹
â€¢ ä¾å­˜é–¢ä¿‚: {len(config['dependencies'])}ä»¶"""
            
    elif action == "set_maintenance":
        host_id = params.get("host_id", "ä¸æ˜")
        duration = params.get("duration_minutes", 60)
        start_time = datetime.now()
        end_time = start_time + timedelta(minutes=duration)
        
        if "maintenance" not in st.session_state:
            st.session_state.maintenance = {}
        st.session_state.maintenance[host_id] = {
            "start": start_time.isoformat(),
            "end": end_time.isoformat(),
            "duration": duration
        }
        
        result["message"] = f"""âœ… {host_id}ã‚’ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šã—ã¾ã—ãŸ
â€¢ é–‹å§‹: {start_time.strftime('%Y-%m-%d %H:%M')}
â€¢ çµ‚äº†: {end_time.strftime('%Y-%m-%d %H:%M')}
â€¢ æœŸé–“: {duration}åˆ†"""
        result["maintenance"] = st.session_state.maintenance[host_id]
        
    elif action == "search_hosts":
        metric = params.get("metric", "cpu")
        operator = params.get("operator", ">")
        value = params.get("value", 80)
        
        hosts = get_hosts_by_condition(metric, operator, value)
        result["hosts"] = hosts
        
        if hosts:
            host_list = "\n".join([f"â€¢ {h['host_id']}: {h['current_value']:.1f}%" for h in hosts])
            result["message"] = f"ğŸ” {len(hosts)}å°è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š\n{host_list}"
        else:
            result["message"] = f"âœ… æ¡ä»¶ã«åˆã†ãƒ›ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆ{metric} {operator} {value}%ï¼‰"
            
    elif action == "get_metrics":
        host_id = params.get("host_id")
        metrics = get_host_metrics(host_id)
        
        if metrics:
            result["metrics"] = metrics
            metrics_lines = []
            for k, v in metrics.items():
                if isinstance(v, float):
                    metrics_lines.append(f"â€¢ {k}: {v:.1f}%")
                else:
                    metrics_lines.append(f"â€¢ {k}: {v}")
            result["message"] = f"ğŸ“Š {host_id}ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼š\n" + "\n".join(metrics_lines)
        else:
            result["message"] = f"âŒ ãƒ›ã‚¹ãƒˆ {host_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            
    elif action == "get_alerts":
        alerts = get_alerts()
        result["alerts"] = alerts
        
        if alerts:
            alert_list = "\n".join([
                f"{'ğŸ”´' if a['severity']=='high' else 'ğŸŸ¡'} {a['host']}: {a['message']}" 
                for a in alerts
            ])
            result["message"] = f"âš ï¸ {len(alerts)}ä»¶ã®ã‚¢ãƒ©ãƒ¼ãƒˆï¼š\n{alert_list}"
        else:
            result["message"] = "âœ… ç¾åœ¨ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“"
            
    elif action == "show_graph":
        host_id = params.get("host_id", "WAN_ROUTER_01")
        metric = params.get("metric", "cpu")
        hours = params.get("hours", 24)
        
        history = generate_metrics_history(host_id, metric, hours)
        result["graph_data"] = history
        result["host_id"] = host_id
        result["metric"] = metric
        
        if history:
            peak = max(history, key=lambda x: x["value"])
            result["message"] = f"ğŸ“ˆ {host_id}ã®{metric}æ¨ç§»ï¼ˆéå»{hours}æ™‚é–“ï¼‰\nãƒ”ãƒ¼ã‚¯: {peak['value']:.1f}% ({peak['timestamp'].strftime('%H:%M')})"
        else:
            result["message"] = f"âŒ {host_id}ã®{metric}ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
    else:
        result["message"] = """ğŸ¤” ã™ã¿ã¾ã›ã‚“ã€æ„å›³ã‚’ç†è§£ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚
        
ä»¥ä¸‹ã®ã‚ˆã†ãªè³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„ï¼š
â€¢ ãƒˆãƒãƒ­ã‚¸ãƒ¼ã§ç›£è¦–è¨­å®šã—ã¦
â€¢ CPU80%è¶…ãˆã¦ã‚‹ã‚µãƒ¼ãƒãƒ¼æ•™ãˆã¦
â€¢ WAN_ROUTER_01ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦‹ã›ã¦
â€¢ ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆæ•™ãˆã¦
â€¢ CORE_SW_01ã®CPUæ¨ç§»ã‚’ã‚°ãƒ©ãƒ•ã§"""
    
    return result

# ==================== UI ====================

def main():
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    col1, col2 = st.columns([3, 1])
    with col1:
        st.markdown('<p class="main-title">ğŸ–¥ï¸ Zabbix AI Assistant</p>', unsafe_allow_html=True)
        st.caption("Powered by gemma-3-12b-it | Demo Mode")
    with col2:
        st.markdown(f"ğŸŸ¢ Online | {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    st.divider()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš¡ ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        
        quick_actions = [
            "ãƒˆãƒãƒ­ã‚¸ãƒ¼ã§ç›£è¦–è¨­å®šã—ã¦",
            "CPU80%è¶…ãˆã¦ã‚‹ã‚µãƒ¼ãƒãƒ¼æ•™ãˆã¦",
            "WAN_ROUTER_01ã‚’30åˆ†ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã«",
            "ç¾åœ¨ã®ã‚¢ãƒ©ãƒ¼ãƒˆæ•™ãˆã¦",
            "CORE_SW_01ã®CPUæ¨ç§»ã‚’ã‚°ãƒ©ãƒ•ã§"
        ]
        
        for action in quick_actions:
            if st.button(action, key=f"quick_{action}", use_container_width=True):
                st.session_state.pending_message = action
        
        st.divider()
        
        # ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        st.header("ğŸ“ ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        uploaded_file = st.file_uploader("JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["json"], key="topology_upload")
        
        if uploaded_file:
            try:
                new_topology = json.load(uploaded_file)
                # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                os.makedirs(DATA_DIR, exist_ok=True)
                with open(os.path.join(DATA_DIR, "topology.json"), "w", encoding="utf-8") as f:
                    json.dump(new_topology, f, ensure_ascii=False, indent=2)
                st.success(f"âœ… {len(new_topology)}å°ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.cache_data.clear()
                st.rerun()
            except Exception as e:
                st.error(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        st.divider()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±
        st.header("ğŸ’¾ ã‚³ãƒãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥")
        cache = get_command_cache()
        st.caption(f"{len(cache)}ä»¶ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸­")
        if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", use_container_width=True):
            st.session_state.command_cache = {}
            st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": """ã“ã‚“ã«ã¡ã¯ï¼Zabbix AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ã‚ˆã†ãªã“ã¨ãŒã§ãã¾ã™ï¼š
â€¢ ãƒˆãƒãƒ­ã‚¸ãƒ¼ã‹ã‚‰ã®è‡ªå‹•ç›£è¦–è¨­å®š
â€¢ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
â€¢ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª
â€¢ ã‚¢ãƒ©ãƒ¼ãƒˆã®ç¢ºèª
â€¢ ã‚°ãƒ©ãƒ•è¡¨ç¤º

ä½•ã‹ãŠæ‰‹ä¼ã„ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ"""
            }
        ]
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            if "data" in message:
                data = message["data"]
                
                # ã‚°ãƒ©ãƒ•è¡¨ç¤º
                if "graph_data" in data and data["graph_data"]:
                    import pandas as pd
                    df = pd.DataFrame(data["graph_data"])
                    df["timestamp"] = pd.to_datetime(df["timestamp"])
                    df = df.set_index("timestamp")
                    st.line_chart(df["value"], use_container_width=True)
                
                # è¨­å®šãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                if "config" in data:
                    with st.expander("ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸè¨­å®šã‚’è¡¨ç¤º"):
                        st.json(data["config"])
                
                # ãƒ›ã‚¹ãƒˆä¸€è¦§
                if "hosts" in data and data["hosts"]:
                    for host in data["hosts"]:
                        severity_color = "ğŸ”´" if host["current_value"] > 90 else "ğŸŸ¡" if host["current_value"] > 80 else "ğŸŸ¢"
                        st.markdown(f"{severity_color} **{host['host_id']}**: {host['current_value']:.1f}%")
    
    # ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
    if "pending_message" in st.session_state:
        pending = st.session_state.pending_message
        del st.session_state.pending_message
        
        st.session_state.messages.append({"role": "user", "content": pending})
        
        with st.chat_message("user"):
            st.markdown(pending)
        
        with st.chat_message("assistant"):
            with st.spinner("å‡¦ç†ä¸­..."):
                result = process_message(pending)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒƒã‚¸
            if result.get("cached"):
                st.markdown("âš¡ **Cached**", help="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¿œç­”ã—ã¾ã—ãŸ")
            
            st.markdown(result.get("message", ""))
            
            # ã‚°ãƒ©ãƒ•è¡¨ç¤º
            if "graph_data" in result and result["graph_data"]:
                import pandas as pd
                df = pd.DataFrame(result["graph_data"])
                df["timestamp"] = pd.to_datetime(df["timestamp"])
                df = df.set_index("timestamp")
                st.line_chart(df["value"], use_container_width=True)
            
            # è¨­å®šãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            if "config" in result:
                with st.expander("ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸè¨­å®šã‚’è¡¨ç¤º"):
                    st.json(result["config"])
            
            # ãƒ›ã‚¹ãƒˆä¸€è¦§
            if "hosts" in result and result["hosts"]:
                for host in result["hosts"]:
                    severity_color = "ğŸ”´" if host["current_value"] > 90 else "ğŸŸ¡" if host["current_value"] > 80 else "ğŸŸ¢"
                    st.markdown(f"{severity_color} **{host['host_id']}**: {host['current_value']:.1f}%")
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": result.get("message", ""),
            "data": result
        })
        st.rerun()
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›... (ä¾‹: CPUé«˜ã„ã‚µãƒ¼ãƒãƒ¼æ•™ãˆã¦)"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("å‡¦ç†ä¸­..."):
                result = process_message(prompt)
            
            if result.get("cached"):
                st.markdown("âš¡ **Cached**", help="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¿œç­”ã—ã¾ã—ãŸ")
            
            st.markdown(result.get("message", ""))
            
            if "graph_data" in result and result["graph_data"]:
                import pandas as pd
                df = pd.DataFrame(result["graph_data"])
                df["timestamp"] = pd.to_datetime(df["timestamp"])
                df = df.set_index("timestamp")
                st.line_chart(df["value"], use_container_width=True)
            
            if "config" in result:
                with st.expander("ğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸè¨­å®šã‚’è¡¨ç¤º"):
                    st.json(result["config"])
            
            if "hosts" in result and result["hosts"]:
                for host in result["hosts"]:
                    severity_color = "ğŸ”´" if host["current_value"] > 90 else "ğŸŸ¡" if host["current_value"] > 80 else "ğŸŸ¢"
                    st.markdown(f"{severity_color} **{host['host_id']}**: {host['current_value']:.1f}%")
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": result.get("message", ""),
            "data": result
        })

if __name__ == "__main__":
    main()
